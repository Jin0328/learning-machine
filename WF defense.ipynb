{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zMOILCPc9MD",
        "outputId": "73241b7f-ee78-41ce-9473-1e0cdbd445d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uty3e4vtz8bB",
        "outputId": "75cec436-43c6-48fe-9af6-cec7a134fb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading monitored datafile...\n",
            "Loading unmonitored datafile...\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Constants\n",
        "USE_SUBLABEL = False\n",
        "URL_PER_SITE = 10\n",
        "TOTAL_URLS_MONITORED = 950\n",
        "TOTAL_URLS_UNMONITORED = 3000\n",
        "\n",
        "# Load monitored data\n",
        "print(\"Loading monitored datafile...\")\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/dataset/mon_standard.pkl\", 'rb') as fi:\n",
        "    monitored_data = pickle.load(fi)\n",
        "\n",
        "X1_mon, X2_mon, y_mon = [], [], []\n",
        "for i in range(TOTAL_URLS_MONITORED):\n",
        "    label = i // URL_PER_SITE\n",
        "    for sample in monitored_data[i]:\n",
        "        size_seq = []\n",
        "        time_seq = []\n",
        "        for c in sample:\n",
        "            dr = 1 if c > 0 else -1\n",
        "            time_seq.append(abs(c))\n",
        "            size_seq.append(dr * 512)\n",
        "        X1_mon.append(time_seq)\n",
        "        X2_mon.append(size_seq)\n",
        "        y_mon.append(label)\n",
        "\n",
        "# Load unmonitored data\n",
        "print(\"Loading unmonitored datafile...\")\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/unmon_standard10_3000.pkl', 'rb') as f:\n",
        "    unmonitored_data = pickle.load(f)\n",
        "\n",
        "X1_unmon, X2_unmon, y_unmon = [], [], []\n",
        "for i in range(TOTAL_URLS_UNMONITORED):\n",
        "    size_seq = []\n",
        "    time_seq = []\n",
        "    for c in unmonitored_data[i]:\n",
        "        dr = 1 if c > 0 else -1\n",
        "        time_seq.append(abs(c))\n",
        "        size_seq.append(dr * 512)\n",
        "    X1_unmon.append(time_seq)\n",
        "    X2_unmon.append(size_seq)\n",
        "    y_unmon.append(-1)\n",
        "\n",
        "# Combine monitored and unmonitored data\n",
        "df_mon = pd.DataFrame({\n",
        "    'time_seq': X1_mon,\n",
        "    'size_seq': X2_mon,\n",
        "    'label': y_mon\n",
        "})\n",
        "df_unmon = pd.DataFrame({\n",
        "    'time_seq': X1_unmon,\n",
        "    'size_seq': X2_unmon,\n",
        "    'label': y_unmon\n",
        "})\n",
        "\n",
        "df_combined = pd.concat([df_mon, df_unmon], ignore_index=True)\n",
        "\n",
        "# Extract Continuous Features\n",
        "df_combined['cumulative_size'] = df_combined['size_seq'].apply(np.cumsum)\n",
        "df_combined['burst_std'] = df_combined['size_seq'].apply(lambda x: np.std(np.diff(x, prepend=0)))\n",
        "df_combined['mean_packet_size'] = df_combined['size_seq'].apply(np.mean)\n",
        "df_combined['mean_timestamp'] = df_combined['time_seq'].apply(np.mean)\n",
        "\n",
        "# Extract Categorical Features\n",
        "df_combined['num_incoming'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i < 0))\n",
        "df_combined['num_outgoing'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i > 0))\n",
        "df_combined['fraction_incoming'] = df_combined['num_incoming'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['fraction_outgoing'] = df_combined['num_outgoing'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['total_packets'] = df_combined['num_incoming'] + df_combined['num_outgoing']\n",
        "\n",
        "# Add Features\n",
        "df_combined['packetsize_std'] = df_combined['size_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['timestamp_std'] = df_combined['time_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['mean_outgoing_packets'] = df_combined['size_seq'].apply(lambda x: np.mean([i for i in x if i > 0]) if any(i > 0 for i in x) else 0)\n",
        "df_combined['packet_concentration_ordering'] = df_combined['size_seq'].apply(lambda x: np.mean(np.diff(x)))\n",
        "\n",
        "# Fill NaN values (if any)\n",
        "df_combined.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. defense 적용 전: 61.95%"
      ],
      "metadata": {
        "id": "seYBAUFigGwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_features = [\n",
        "    'cumulative_size', 'burst_std', 'mean_packet_size', 'mean_timestamp',\n",
        "    'packetsize_std', 'timestamp_std']\n",
        "categorical_features = ['fraction_incoming', 'fraction_outgoing', 'total_packets', 'num_incoming', 'num_outgoing',\n",
        "                        'mean_outgoing_packets', 'packet_concentration_ordering']\n",
        "\n",
        "# Flatten lists\n",
        "X_continuous = np.array(df_combined[continuous_features].map(lambda x: np.mean(x) if isinstance(x, np.ndarray) else x))\n",
        "X_categorical = df_combined[categorical_features].values\n",
        "X = np.hstack([X_continuous, X_categorical])\n",
        "y = df_combined['label'].values\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize Continuous Features\n",
        "scaler = StandardScaler()\n",
        "X_train[:, :len(continuous_features)] = scaler.fit_transform(X_train[:, :len(continuous_features)])\n",
        "X_test[:, :len(continuous_features)] = scaler.transform(X_test[:, :len(continuous_features)])\n",
        "\n",
        "# 랜덤 포레스트 모델 정의\n",
        "clf = RandomForestClassifier(random_state=123)\n",
        "\n",
        "# 모델 훈련\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 수행\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 평가 지표 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW1AjFo7f83j",
        "outputId": "bc44175f-2663-4df4-b1cd-85bd178b7b30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 61.95%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.43      0.53      0.48       882\n",
            "           0       0.67      0.55      0.60        58\n",
            "           1       0.46      0.43      0.44        60\n",
            "           2       0.76      0.77      0.77        62\n",
            "           3       0.65      0.65      0.65        46\n",
            "           4       0.58      0.76      0.66        50\n",
            "           5       0.86      0.82      0.84        60\n",
            "           6       0.77      0.76      0.77        63\n",
            "           7       0.49      0.70      0.58        50\n",
            "           8       0.64      0.72      0.68        54\n",
            "           9       0.53      0.58      0.55        48\n",
            "          10       0.64      0.54      0.59        67\n",
            "          11       0.64      0.59      0.62        64\n",
            "          12       0.79      0.87      0.83        61\n",
            "          13       0.54      0.28      0.37        76\n",
            "          14       0.51      0.39      0.44        57\n",
            "          15       0.67      0.79      0.73        52\n",
            "          16       0.78      0.68      0.73        66\n",
            "          17       0.71      0.52      0.60        71\n",
            "          18       0.83      0.88      0.85        57\n",
            "          19       0.66      0.66      0.66        59\n",
            "          20       0.89      0.92      0.90        62\n",
            "          21       0.39      0.31      0.34        55\n",
            "          22       0.52      0.44      0.47        62\n",
            "          23       0.63      0.72      0.67        57\n",
            "          24       0.42      0.26      0.32        58\n",
            "          25       0.70      0.51      0.59        59\n",
            "          26       0.70      0.83      0.76        58\n",
            "          27       0.73      0.53      0.61        66\n",
            "          28       0.82      0.73      0.77        49\n",
            "          29       0.57      0.67      0.61        64\n",
            "          30       0.73      0.58      0.65        65\n",
            "          31       0.63      0.59      0.61        68\n",
            "          32       0.55      0.49      0.52        68\n",
            "          33       0.72      0.77      0.74        64\n",
            "          34       0.35      0.24      0.29        58\n",
            "          35       0.63      0.75      0.68        51\n",
            "          36       0.59      0.69      0.63        54\n",
            "          37       0.67      0.44      0.53        68\n",
            "          38       0.70      0.59      0.64        76\n",
            "          39       0.80      0.68      0.73        65\n",
            "          40       0.71      0.63      0.67        65\n",
            "          41       0.80      0.75      0.77        71\n",
            "          42       0.45      0.42      0.44        64\n",
            "          43       0.82      0.78      0.80        65\n",
            "          44       0.89      0.97      0.93        65\n",
            "          45       0.53      0.55      0.54        58\n",
            "          46       0.72      0.55      0.63        56\n",
            "          47       0.52      0.41      0.46        70\n",
            "          48       0.70      0.63      0.66        59\n",
            "          49       0.74      0.75      0.75        57\n",
            "          50       0.55      0.56      0.56        55\n",
            "          51       0.52      0.45      0.48        65\n",
            "          52       0.74      0.70      0.72        76\n",
            "          53       0.63      0.67      0.65        55\n",
            "          54       0.70      0.77      0.73        52\n",
            "          55       0.53      0.41      0.46        59\n",
            "          56       0.93      0.92      0.92        60\n",
            "          57       0.65      0.69      0.67        61\n",
            "          58       0.77      0.91      0.83        64\n",
            "          59       0.62      0.91      0.74        54\n",
            "          60       0.52      0.66      0.58        53\n",
            "          61       0.57      0.60      0.59        53\n",
            "          62       0.58      0.64      0.61        59\n",
            "          63       0.40      0.37      0.39        62\n",
            "          64       0.60      0.59      0.59        63\n",
            "          65       0.55      0.62      0.58        55\n",
            "          66       0.69      0.71      0.70        52\n",
            "          67       0.81      0.83      0.82        69\n",
            "          68       0.51      0.39      0.44        59\n",
            "          69       0.45      0.37      0.41        59\n",
            "          70       0.91      0.95      0.93        65\n",
            "          71       0.51      0.52      0.52        44\n",
            "          72       0.76      0.64      0.70        73\n",
            "          73       0.66      0.74      0.69        68\n",
            "          74       0.42      0.59      0.49        54\n",
            "          75       0.89      0.90      0.90        62\n",
            "          76       0.93      0.86      0.89        59\n",
            "          77       0.44      0.32      0.37        56\n",
            "          78       0.39      0.38      0.38        48\n",
            "          79       0.60      0.40      0.48        72\n",
            "          80       0.80      0.81      0.80        63\n",
            "          81       0.53      0.57      0.55        60\n",
            "          82       0.77      0.56      0.65        59\n",
            "          83       0.71      0.58      0.64        62\n",
            "          84       0.56      0.60      0.58        58\n",
            "          85       0.76      0.82      0.79        65\n",
            "          86       0.86      0.98      0.92        57\n",
            "          87       0.85      0.69      0.76        65\n",
            "          88       0.60      0.71      0.65        51\n",
            "          89       0.52      0.47      0.49        58\n",
            "          90       0.63      0.61      0.62        66\n",
            "          91       0.65      0.58      0.62        55\n",
            "          92       0.45      0.52      0.48        46\n",
            "          93       0.80      0.93      0.86        75\n",
            "          94       0.48      0.56      0.52        54\n",
            "\n",
            "    accuracy                           0.62      6600\n",
            "   macro avg       0.65      0.63      0.63      6600\n",
            "weighted avg       0.62      0.62      0.62      6600\n",
            "\n",
            "Confusion Matrix:\n",
            "[[466   4   5 ...   6   2   9]\n",
            " [  8  32   0 ...   1   0   1]\n",
            " [  2   0  26 ...   0   1   0]\n",
            " ...\n",
            " [ 14   0   0 ...  24   0   1]\n",
            " [  3   0   0 ...   0  70   0]\n",
            " [  5   0   0 ...   2   1  30]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WF defense 적용"
      ],
      "metadata": {
        "id": "g9Wb-yVd2CjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. traffic split만 적용: 56.94%"
      ],
      "metadata": {
        "id": "MHZfxSsScSHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined = pd.concat([df_mon, df_unmon], ignore_index=True)\n",
        "\n",
        "df_combined['cumulative_size'] = df_combined['size_seq'].apply(np.cumsum)\n",
        "df_combined['burst_std'] = df_combined['size_seq'].apply(lambda x: np.std(np.diff(x, prepend=0)))\n",
        "df_combined['mean_packet_size'] = df_combined['size_seq'].apply(np.mean)\n",
        "df_combined['mean_timestamp'] = df_combined['time_seq'].apply(np.mean)\n",
        "df_combined['num_incoming'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i < 0))\n",
        "df_combined['num_outgoing'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i > 0))\n",
        "df_combined['fraction_incoming'] = df_combined['num_incoming'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['fraction_outgoing'] = df_combined['num_outgoing'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['total_packets'] = df_combined['num_incoming'] + df_combined['num_outgoing']\n",
        "df_combined['packetsize_std'] = df_combined['size_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['timestamp_std'] = df_combined['time_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['mean_outgoing_packets'] = df_combined['size_seq'].apply(lambda x: np.mean([i for i in x if i > 0]) if any(i > 0 for i in x) else 0)\n",
        "df_combined['packet_concentration_ordering'] = df_combined['size_seq'].apply(lambda x: np.mean(np.diff(x)))\n",
        "\n",
        "# Apply traffic splitting defense only\n",
        "def traffic_split_only(df, max_splits=3):\n",
        "    \"\"\"Applying WF defense mechanisms: only traffic split...\"\"\"\n",
        "    def defense_pipeline(row):\n",
        "        split_size, split_time = split_traffic(row['size_seq'], row['time_seq'], max_splits)\n",
        "        return split_size, split_time\n",
        "\n",
        "    df['size_seq'], df['time_seq'] = zip(*df.apply(defense_pipeline, axis=1))\n",
        "    return df\n",
        "\n",
        "# Defining the helper defense function for traffic splitting\n",
        "def split_traffic(size_seq, time_seq, max_splits=3):\n",
        "    split_points = sorted(random.sample(range(1, len(size_seq)), random.randint(1, max_splits)))\n",
        "    size_splits = [size_seq[i:j] for i, j in zip([0] + split_points, split_points + [len(size_seq)])]\n",
        "    time_splits = [time_seq[i:j] for i, j in zip([0] + split_points, split_points + [len(time_seq)])]\n",
        "    # Flatten and shuffle the segments to break sequence patterns\n",
        "    shuffled_indices = list(range(len(size_splits)))\n",
        "    random.shuffle(shuffled_indices)\n",
        "    size_shuffled = [size for idx in shuffled_indices for size in size_splits[idx]]\n",
        "    time_shuffled = [time for idx in shuffled_indices for time in time_splits[idx]]\n",
        "    return size_shuffled, time_shuffled\n",
        "\n",
        "# Apply the traffic split defense function to the combined dataset\n",
        "df_combined = traffic_split_only(df_combined)\n",
        "\n",
        "# Extract additional features\n",
        "df_combined['cumulative_size'] = df_combined['size_seq'].apply(np.cumsum)\n",
        "df_combined['burst_std'] = df_combined['size_seq'].apply(lambda x: np.std(np.diff(x, prepend=0)))\n",
        "df_combined['mean_packet_size'] = df_combined['size_seq'].apply(np.mean)\n",
        "df_combined['mean_timestamp'] = df_combined['time_seq'].apply(np.mean)\n",
        "df_combined['num_incoming'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i < 0))\n",
        "df_combined['num_outgoing'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i > 0))\n",
        "df_combined['fraction_incoming'] = df_combined['num_incoming'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['fraction_outgoing'] = df_combined['num_outgoing'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['total_packets'] = df_combined['num_incoming'] + df_combined['num_outgoing']\n",
        "\n",
        "df_combined['packetsize_std'] = df_combined['size_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['timestamp_std'] = df_combined['time_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['mean_outgoing_packets'] = df_combined['size_seq'].apply(lambda x: np.mean([i for i in x if i > 0]) if any(i > 0 for i in x) else 0)\n",
        "df_combined['packet_concentration_ordering'] = df_combined['size_seq'].apply(lambda x: np.mean(np.diff(x)))\n",
        "\n",
        "# Extract the features for training\n",
        "continuous_features = [\n",
        "    'cumulative_size', 'burst_std', 'mean_packet_size', 'mean_timestamp',\n",
        "    'packetsize_std', 'timestamp_std']\n",
        "categorical_features = ['fraction_incoming', 'fraction_outgoing', 'total_packets', 'num_incoming', 'num_outgoing',\n",
        "                        'mean_outgoing_packets', 'packet_concentration_ordering']\n",
        "\n",
        "X_continuous = np.array(df_combined[continuous_features].map(lambda x: np.mean(x) if isinstance(x, np.ndarray) else x))\n",
        "X_categorical = df_combined[categorical_features].values\n",
        "X = np.hstack([X_continuous, X_categorical])\n",
        "y = df_combined['label'].values\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fill NaN values (if any)\n",
        "df_combined.fillna(0, inplace=True)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train[:, :len(continuous_features)] = scaler.fit_transform(X_train[:, :len(continuous_features)])\n",
        "X_test[:, :len(continuous_features)] = scaler.transform(X_test[:, :len(continuous_features)])\n",
        "\n",
        "# 랜덤 포레스트 모델 정의\n",
        "clf = RandomForestClassifier(random_state=123)\n",
        "\n",
        "# 모델 훈련\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 수행\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 평가 지표 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft6wYgyBcZqr",
        "outputId": "983122c0-589b-485f-f13e-50ecf02c03f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 56.94%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.42      0.50      0.46       882\n",
            "           0       0.52      0.45      0.48        58\n",
            "           1       0.40      0.35      0.38        60\n",
            "           2       0.69      0.73      0.71        62\n",
            "           3       0.49      0.52      0.51        46\n",
            "           4       0.48      0.68      0.56        50\n",
            "           5       0.73      0.77      0.75        60\n",
            "           6       0.75      0.68      0.72        63\n",
            "           7       0.49      0.68      0.57        50\n",
            "           8       0.58      0.57      0.58        54\n",
            "           9       0.46      0.50      0.48        48\n",
            "          10       0.59      0.45      0.51        67\n",
            "          11       0.67      0.55      0.60        64\n",
            "          12       0.73      0.87      0.79        61\n",
            "          13       0.47      0.25      0.33        76\n",
            "          14       0.44      0.37      0.40        57\n",
            "          15       0.59      0.77      0.67        52\n",
            "          16       0.72      0.64      0.68        66\n",
            "          17       0.50      0.37      0.42        71\n",
            "          18       0.84      0.82      0.83        57\n",
            "          19       0.66      0.69      0.68        59\n",
            "          20       0.88      0.90      0.89        62\n",
            "          21       0.29      0.27      0.28        55\n",
            "          22       0.39      0.35      0.37        62\n",
            "          23       0.56      0.61      0.59        57\n",
            "          24       0.33      0.22      0.27        58\n",
            "          25       0.69      0.53      0.60        59\n",
            "          26       0.71      0.86      0.78        58\n",
            "          27       0.64      0.52      0.57        66\n",
            "          28       0.78      0.73      0.76        49\n",
            "          29       0.49      0.59      0.54        64\n",
            "          30       0.67      0.58      0.62        65\n",
            "          31       0.59      0.57      0.58        68\n",
            "          32       0.42      0.44      0.43        68\n",
            "          33       0.73      0.77      0.75        64\n",
            "          34       0.25      0.16      0.19        58\n",
            "          35       0.49      0.69      0.57        51\n",
            "          36       0.50      0.50      0.50        54\n",
            "          37       0.53      0.28      0.37        68\n",
            "          38       0.60      0.51      0.55        76\n",
            "          39       0.56      0.52      0.54        65\n",
            "          40       0.66      0.54      0.59        65\n",
            "          41       0.79      0.65      0.71        71\n",
            "          42       0.40      0.34      0.37        64\n",
            "          43       0.78      0.69      0.73        65\n",
            "          44       0.89      0.97      0.93        65\n",
            "          45       0.52      0.55      0.54        58\n",
            "          46       0.66      0.45      0.53        56\n",
            "          47       0.47      0.33      0.39        70\n",
            "          48       0.64      0.51      0.57        59\n",
            "          49       0.67      0.61      0.64        57\n",
            "          50       0.46      0.53      0.49        55\n",
            "          51       0.53      0.48      0.50        65\n",
            "          52       0.61      0.61      0.61        76\n",
            "          53       0.64      0.64      0.64        55\n",
            "          54       0.66      0.77      0.71        52\n",
            "          55       0.42      0.37      0.40        59\n",
            "          56       0.90      0.90      0.90        60\n",
            "          57       0.56      0.57      0.57        61\n",
            "          58       0.76      0.89      0.82        64\n",
            "          59       0.63      0.83      0.72        54\n",
            "          60       0.47      0.62      0.54        53\n",
            "          61       0.52      0.62      0.56        53\n",
            "          62       0.60      0.61      0.61        59\n",
            "          63       0.43      0.32      0.37        62\n",
            "          64       0.49      0.51      0.50        63\n",
            "          65       0.49      0.55      0.52        55\n",
            "          66       0.54      0.69      0.61        52\n",
            "          67       0.77      0.81      0.79        69\n",
            "          68       0.50      0.39      0.44        59\n",
            "          69       0.42      0.42      0.42        59\n",
            "          70       0.91      0.92      0.92        65\n",
            "          71       0.45      0.50      0.47        44\n",
            "          72       0.77      0.59      0.67        73\n",
            "          73       0.59      0.75      0.66        68\n",
            "          74       0.39      0.52      0.44        54\n",
            "          75       0.75      0.85      0.80        62\n",
            "          76       0.94      0.86      0.90        59\n",
            "          77       0.39      0.25      0.30        56\n",
            "          78       0.35      0.35      0.35        48\n",
            "          79       0.48      0.33      0.39        72\n",
            "          80       0.72      0.79      0.76        63\n",
            "          81       0.39      0.40      0.39        60\n",
            "          82       0.72      0.56      0.63        59\n",
            "          83       0.64      0.55      0.59        62\n",
            "          84       0.45      0.52      0.48        58\n",
            "          85       0.73      0.74      0.73        65\n",
            "          86       0.82      0.95      0.88        57\n",
            "          87       0.88      0.65      0.74        65\n",
            "          88       0.53      0.67      0.59        51\n",
            "          89       0.48      0.43      0.45        58\n",
            "          90       0.63      0.48      0.55        66\n",
            "          91       0.62      0.58      0.60        55\n",
            "          92       0.38      0.43      0.40        46\n",
            "          93       0.79      0.85      0.82        75\n",
            "          94       0.42      0.43      0.42        54\n",
            "\n",
            "    accuracy                           0.57      6600\n",
            "   macro avg       0.59      0.58      0.58      6600\n",
            "weighted avg       0.57      0.57      0.57      6600\n",
            "\n",
            "Confusion Matrix:\n",
            "[[445   5   4 ...   8   1   7]\n",
            " [ 10  26   0 ...   0   0   0]\n",
            " [  1   0  21 ...   0   1   0]\n",
            " ...\n",
            " [ 11   0   0 ...  20   0   2]\n",
            " [  2   1   0 ...   0  64   0]\n",
            " [  7   0   0 ...   1   1  23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. padding, random delay, traffic split만 적용: 52.95%"
      ],
      "metadata": {
        "id": "XkazhMcRBJac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# WF Defense mechanisms\n",
        "def apply_padding(size_seq, max_padding=512):\n",
        "    \"\"\"Apply random padding to packet sizes.\"\"\"\n",
        "    return [s + random.randint(-max_padding, max_padding) for s in size_seq]\n",
        "\n",
        "def insert_random_delays(time_seq, max_delay=100):\n",
        "    \"\"\"Insert random delays into time sequence.\"\"\"\n",
        "    return [t + random.uniform(0, max_delay) for t in time_seq]\n",
        "\n",
        "def split_traffic(size_seq, time_seq, max_splits=3):\n",
        "    \"\"\"Randomly split traffic into multiple smaller segments.\"\"\"\n",
        "    split_points = sorted(random.sample(range(1, len(size_seq)), random.randint(1, max_splits)))\n",
        "    size_splits = [size_seq[i:j] for i, j in zip([0] + split_points, split_points + [len(size_seq)])]\n",
        "    time_splits = [time_seq[i:j] for i, j in zip([0] + split_points, split_points + [len(time_seq)])]\n",
        "    # Flatten and shuffle the segments to break sequence patterns\n",
        "    shuffled_indices = list(range(len(size_splits)))\n",
        "    random.shuffle(shuffled_indices)\n",
        "    size_shuffled = [size for idx in shuffled_indices for size in size_splits[idx]]\n",
        "    time_shuffled = [time for idx in shuffled_indices for time in time_splits[idx]]\n",
        "    return size_shuffled, time_shuffled\n",
        "\n",
        "def perturb_data_with_all_defenses(df, max_padding=512, max_delay=100, max_splits=3):\n",
        "    \"\"\"Apply all WF defenses: padding, delays, and traffic splitting.\"\"\"\n",
        "    df['size_seq'], df['time_seq'] = zip(*df.apply(\n",
        "        lambda row: split_traffic(\n",
        "            apply_padding(row['size_seq'], max_padding),\n",
        "            insert_random_delays(row['time_seq'], max_delay),\n",
        "            max_splits=max_splits\n",
        "        ), axis=1))\n",
        "    return df\n",
        "\n",
        "# Apply all WF defenses to the combined dataset\n",
        "print(\"Applying WF defense mechanisms: padding, delays, and traffic splitting...\")\n",
        "df_combined = perturb_data_with_all_defenses(df_combined, max_padding=256, max_delay=50, max_splits=3)\n",
        "\n",
        "# Recompute features after applying all defenses\n",
        "df_combined['cumulative_size'] = df_combined['size_seq'].apply(np.cumsum)\n",
        "df_combined['burst_std'] = df_combined['size_seq'].apply(lambda x: np.std(np.diff(x, prepend=0)))\n",
        "df_combined['mean_packet_size'] = df_combined['size_seq'].apply(np.mean)\n",
        "df_combined['mean_timestamp'] = df_combined['time_seq'].apply(np.mean)\n",
        "df_combined['num_incoming'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i < 0))\n",
        "df_combined['num_outgoing'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i > 0))\n",
        "df_combined['fraction_incoming'] = df_combined['num_incoming'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['fraction_outgoing'] = df_combined['num_outgoing'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['total_packets'] = df_combined['num_incoming'] + df_combined['num_outgoing']\n",
        "df_combined['packetsize_std'] = df_combined['size_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['timestamp_std'] = df_combined['time_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['mean_outgoing_packets'] = df_combined['size_seq'].apply(lambda x: np.mean([i for i in x if i > 0]) if any(i > 0 for i in x) else 0)\n",
        "df_combined['packet_concentration_ordering'] = df_combined['size_seq'].apply(lambda x: np.mean(np.diff(x)))\n",
        "\n",
        "# Fill NaN values (if any)\n",
        "df_combined.fillna(0, inplace=True)\n",
        "\n",
        "# Extract the features again\n",
        "X_continuous = np.array(df_combined[continuous_features].map(lambda x: np.mean(x) if isinstance(x, np.ndarray) else x))\n",
        "X_categorical = df_combined[categorical_features].values\n",
        "X = np.hstack([X_continuous, X_categorical])\n",
        "y = df_combined['label'].values\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train[:, :len(continuous_features)] = scaler.fit_transform(X_train[:, :len(continuous_features)])\n",
        "X_test[:, :len(continuous_features)] = scaler.transform(X_test[:, :len(continuous_features)])\n",
        "\n",
        "# 랜덤 포레스트 모델 정의\n",
        "clf = RandomForestClassifier(random_state=123)\n",
        "\n",
        "# 모델 훈련\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 수행\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 평가 지표 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aa6uFror9sV3",
        "outputId": "7eaa1610-cbc3-4ddf-e7d6-e99bb1b804db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying WF defense mechanisms: padding, delays, and traffic splitting...\n",
            "Accuracy: 52.95%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.37      0.48      0.42       882\n",
            "           0       0.47      0.33      0.39        58\n",
            "           1       0.33      0.28      0.30        60\n",
            "           2       0.68      0.77      0.72        62\n",
            "           3       0.49      0.54      0.52        46\n",
            "           4       0.44      0.66      0.53        50\n",
            "           5       0.71      0.75      0.73        60\n",
            "           6       0.71      0.59      0.64        63\n",
            "           7       0.48      0.78      0.59        50\n",
            "           8       0.62      0.54      0.57        54\n",
            "           9       0.37      0.48      0.41        48\n",
            "          10       0.57      0.42      0.48        67\n",
            "          11       0.55      0.42      0.48        64\n",
            "          12       0.71      0.87      0.78        61\n",
            "          13       0.36      0.17      0.23        76\n",
            "          14       0.40      0.33      0.36        57\n",
            "          15       0.59      0.81      0.68        52\n",
            "          16       0.77      0.67      0.72        66\n",
            "          17       0.49      0.32      0.39        71\n",
            "          18       0.76      0.88      0.81        57\n",
            "          19       0.61      0.64      0.63        59\n",
            "          20       0.86      0.89      0.87        62\n",
            "          21       0.37      0.20      0.26        55\n",
            "          22       0.33      0.27      0.30        62\n",
            "          23       0.51      0.63      0.57        57\n",
            "          24       0.37      0.17      0.24        58\n",
            "          25       0.59      0.39      0.47        59\n",
            "          26       0.57      0.74      0.64        58\n",
            "          27       0.69      0.41      0.51        66\n",
            "          28       0.69      0.67      0.68        49\n",
            "          29       0.46      0.52      0.49        64\n",
            "          30       0.76      0.48      0.58        65\n",
            "          31       0.50      0.43      0.46        68\n",
            "          32       0.35      0.37      0.36        68\n",
            "          33       0.82      0.77      0.79        64\n",
            "          34       0.31      0.21      0.25        58\n",
            "          35       0.49      0.69      0.57        51\n",
            "          36       0.48      0.39      0.43        54\n",
            "          37       0.44      0.22      0.29        68\n",
            "          38       0.51      0.41      0.45        76\n",
            "          39       0.54      0.52      0.53        65\n",
            "          40       0.54      0.38      0.45        65\n",
            "          41       0.65      0.61      0.63        71\n",
            "          42       0.47      0.33      0.39        64\n",
            "          43       0.71      0.65      0.68        65\n",
            "          44       0.87      0.94      0.90        65\n",
            "          45       0.44      0.45      0.44        58\n",
            "          46       0.62      0.38      0.47        56\n",
            "          47       0.49      0.27      0.35        70\n",
            "          48       0.57      0.49      0.53        59\n",
            "          49       0.65      0.61      0.63        57\n",
            "          50       0.56      0.53      0.54        55\n",
            "          51       0.43      0.40      0.42        65\n",
            "          52       0.64      0.62      0.63        76\n",
            "          53       0.63      0.62      0.62        55\n",
            "          54       0.57      0.63      0.60        52\n",
            "          55       0.47      0.42      0.45        59\n",
            "          56       0.87      0.87      0.87        60\n",
            "          57       0.54      0.56      0.55        61\n",
            "          58       0.70      0.81      0.75        64\n",
            "          59       0.56      0.78      0.65        54\n",
            "          60       0.45      0.64      0.53        53\n",
            "          61       0.43      0.53      0.47        53\n",
            "          62       0.57      0.47      0.52        59\n",
            "          63       0.36      0.32      0.34        62\n",
            "          64       0.50      0.54      0.52        63\n",
            "          65       0.45      0.55      0.49        55\n",
            "          66       0.48      0.63      0.55        52\n",
            "          67       0.81      0.78      0.79        69\n",
            "          68       0.46      0.32      0.38        59\n",
            "          69       0.33      0.37      0.35        59\n",
            "          70       0.94      0.91      0.92        65\n",
            "          71       0.40      0.39      0.39        44\n",
            "          72       0.62      0.55      0.58        73\n",
            "          73       0.58      0.71      0.64        68\n",
            "          74       0.34      0.41      0.37        54\n",
            "          75       0.77      0.89      0.83        62\n",
            "          76       0.94      0.86      0.90        59\n",
            "          77       0.39      0.25      0.30        56\n",
            "          78       0.33      0.23      0.27        48\n",
            "          79       0.44      0.35      0.39        72\n",
            "          80       0.68      0.68      0.68        63\n",
            "          81       0.35      0.43      0.39        60\n",
            "          82       0.82      0.53      0.64        59\n",
            "          83       0.65      0.52      0.58        62\n",
            "          84       0.36      0.45      0.40        58\n",
            "          85       0.70      0.71      0.70        65\n",
            "          86       0.74      0.96      0.84        57\n",
            "          87       0.77      0.68      0.72        65\n",
            "          88       0.49      0.51      0.50        51\n",
            "          89       0.43      0.40      0.41        58\n",
            "          90       0.64      0.55      0.59        66\n",
            "          91       0.55      0.51      0.53        55\n",
            "          92       0.30      0.33      0.31        46\n",
            "          93       0.74      0.76      0.75        75\n",
            "          94       0.32      0.35      0.34        54\n",
            "\n",
            "    accuracy                           0.53      6600\n",
            "   macro avg       0.55      0.54      0.54      6600\n",
            "weighted avg       0.54      0.53      0.52      6600\n",
            "\n",
            "Confusion Matrix:\n",
            "[[426   5   5 ...   5   1  13]\n",
            " [ 16  19   0 ...   1   0   0]\n",
            " [  5   0  17 ...   1   1   0]\n",
            " ...\n",
            " [ 12   0   0 ...  15   0   3]\n",
            " [  2   1   0 ...   0  57   0]\n",
            " [ 12   0   0 ...   2   1  19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. padding, random delay, traffic split, dummy_traffic , time_warping, uniform_packet_size 적용: 33.18%"
      ],
      "metadata": {
        "id": "Nhz8kfWlBaPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply enhanced defense with traffic splitting\n",
        "def enhanced_defense_with_split(df, max_splits=3, uniform_size=512, dummy_prob=0.3, time_scale_range=(0.5, 1.5)):\n",
        "    \"\"\"Applying WF defense mechanisms: padding, random delay, traffic split, dummy_traffic , time_warping, uniform_packet_size...\"\"\"\n",
        "    def defense_pipeline(row):\n",
        "        # 1. 패킷 크기 균일화\n",
        "        uniformed = uniform_packet_size(row['size_seq'], uniform_size)\n",
        "        # 2. 더미 트래픽 삽입\n",
        "        dummy_added = add_dummy_traffic(uniformed, dummy_prob)\n",
        "        # 3. 트래픽 분할\n",
        "        split_size, split_time = split_traffic(dummy_added, row['time_seq'], max_splits)\n",
        "        # 4. 랜덤 시간 왜곡\n",
        "        warped_time = time_warping(split_time, time_scale_range)\n",
        "        return split_size, warped_time\n",
        "\n",
        "    df['size_seq'], df['time_seq'] = zip(*df.apply(defense_pipeline, axis=1))\n",
        "    return df\n",
        "\n",
        "# Defining the helper defense functions\n",
        "\n",
        "import random\n",
        "\n",
        "def apply_padding(size_seq, max_padding=512):\n",
        "    \"\"\"Apply random padding to packet sizes.\"\"\"\n",
        "    return [s + random.randint(-max_padding, max_padding) for s in size_seq]\n",
        "\n",
        "def insert_random_delays(time_seq, max_delay=100):\n",
        "    \"\"\"Insert random delays into time sequence.\"\"\"\n",
        "    return [t + random.uniform(0, max_delay) for t in time_seq]\n",
        "\n",
        "def split_traffic(size_seq, time_seq, max_splits=3):\n",
        "    \"\"\"Randomly split traffic into multiple smaller segments.\"\"\"\n",
        "    split_points = sorted(random.sample(range(1, len(size_seq)), random.randint(1, max_splits)))\n",
        "    size_splits = [size_seq[i:j] for i, j in zip([0] + split_points, split_points + [len(size_seq)])]\n",
        "    time_splits = [time_seq[i:j] for i, j in zip([0] + split_points, split_points + [len(time_seq)])]\n",
        "    # Flatten and shuffle the segments to break sequence patterns\n",
        "    shuffled_indices = list(range(len(size_splits)))\n",
        "    random.shuffle(shuffled_indices)\n",
        "    size_shuffled = [size for idx in shuffled_indices for size in size_splits[idx]]\n",
        "    time_shuffled = [time for idx in shuffled_indices for time in time_splits[idx]]\n",
        "    return size_shuffled, time_shuffled\n",
        "\n",
        "def add_dummy_traffic(size_seq, dummy_prob=0.3):\n",
        "    \"\"\"Insert dummy packets into the sequence.\"\"\"\n",
        "    result = []\n",
        "    for size in size_seq:\n",
        "        result.append(size)\n",
        "        if random.random() < dummy_prob:\n",
        "            result.append(random.choice([-512, 512]))\n",
        "    return result\n",
        "\n",
        "def uniform_packet_size(size_seq, uniform_size=512):\n",
        "    \"\"\"Normalize all packet sizes to a uniform value.\"\"\"\n",
        "    return [uniform_size if s > 0 else -uniform_size for s in size_seq]\n",
        "\n",
        "def time_warping(time_seq, scale_factor_range=(0.5, 1.5)):\n",
        "    \"\"\"Apply random time warping to distort timestamp patterns.\"\"\"\n",
        "    scale_factor = random.uniform(*scale_factor_range)\n",
        "    return [t * scale_factor for t in time_seq]\n",
        "\n",
        "# Apply the enhanced defense function to the combined dataset\n",
        "df_combined = enhanced_defense_with_split(df_combined)\n",
        "\n",
        "# Extract additional features\n",
        "df_combined['cumulative_size'] = df_combined['size_seq'].apply(np.cumsum)\n",
        "df_combined['burst_std'] = df_combined['size_seq'].apply(lambda x: np.std(np.diff(x, prepend=0)))\n",
        "df_combined['mean_packet_size'] = df_combined['size_seq'].apply(np.mean)\n",
        "df_combined['mean_timestamp'] = df_combined['time_seq'].apply(np.mean)\n",
        "df_combined['num_incoming'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i < 0))\n",
        "df_combined['num_outgoing'] = df_combined['size_seq'].apply(lambda x: sum(1 for i in x if i > 0))\n",
        "df_combined['fraction_incoming'] = df_combined['num_incoming'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['fraction_outgoing'] = df_combined['num_outgoing'] / (df_combined['num_incoming'] + df_combined['num_outgoing'])\n",
        "df_combined['total_packets'] = df_combined['num_incoming'] + df_combined['num_outgoing']\n",
        "df_combined['packetsize_std'] = df_combined['size_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['timestamp_std'] = df_combined['time_seq'].apply(lambda x: np.std(x))\n",
        "df_combined['mean_outgoing_packets'] = df_combined['size_seq'].apply(lambda x: np.mean([i for i in x if i > 0]) if any(i > 0 for i in x) else 0)\n",
        "df_combined['packet_concentration_ordering'] = df_combined['size_seq'].apply(lambda x: np.mean(np.diff(x)))\n",
        "\n",
        "# Extract the features for training\n",
        "continuous_features = [\n",
        "    'cumulative_size', 'burst_std', 'mean_packet_size', 'mean_timestamp',\n",
        "    'packetsize_std', 'timestamp_std']\n",
        "categorical_features = ['fraction_incoming', 'fraction_outgoing', 'total_packets', 'num_incoming', 'num_outgoing',\n",
        "                        'mean_outgoing_packets', 'packet_concentration_ordering']\n",
        "\n",
        "X_continuous = np.array(df_combined[continuous_features].map(lambda x: np.mean(x) if isinstance(x, np.ndarray) else x))\n",
        "X_categorical = df_combined[categorical_features].values\n",
        "X = np.hstack([X_continuous, X_categorical])\n",
        "y = df_combined['label'].values\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fill NaN values (if any)\n",
        "df_combined.fillna(0, inplace=True)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train[:, :len(continuous_features)] = scaler.fit_transform(X_train[:, :len(continuous_features)])\n",
        "X_test[:, :len(continuous_features)] = scaler.transform(X_test[:, :len(continuous_features)])\n",
        "\n",
        "# 랜덤 포레스트 모델 정의\n",
        "clf = RandomForestClassifier(random_state=123)\n",
        "\n",
        "# 모델 훈련\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 수행\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 평가 지표 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3VJiUTuGSIA",
        "outputId": "2512ce61-f379-4613-f9cf-2b08e23a6fbd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 33.18%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.22      0.32      0.26       882\n",
            "           0       0.31      0.19      0.24        58\n",
            "           1       0.12      0.13      0.13        60\n",
            "           2       0.45      0.63      0.52        62\n",
            "           3       0.22      0.24      0.23        46\n",
            "           4       0.20      0.36      0.25        50\n",
            "           5       0.48      0.37      0.42        60\n",
            "           6       0.37      0.44      0.40        63\n",
            "           7       0.22      0.32      0.26        50\n",
            "           8       0.48      0.43      0.45        54\n",
            "           9       0.16      0.23      0.19        48\n",
            "          10       0.46      0.27      0.34        67\n",
            "          11       0.27      0.27      0.27        64\n",
            "          12       0.63      0.67      0.65        61\n",
            "          13       0.22      0.11      0.14        76\n",
            "          14       0.37      0.26      0.31        57\n",
            "          15       0.42      0.56      0.48        52\n",
            "          16       0.83      0.59      0.69        66\n",
            "          17       0.11      0.04      0.06        71\n",
            "          18       0.50      0.53      0.51        57\n",
            "          19       0.11      0.08      0.10        59\n",
            "          20       0.67      0.68      0.67        62\n",
            "          21       0.15      0.09      0.11        55\n",
            "          22       0.20      0.18      0.19        62\n",
            "          23       0.39      0.46      0.42        57\n",
            "          24       0.07      0.03      0.05        58\n",
            "          25       0.33      0.22      0.27        59\n",
            "          26       0.41      0.59      0.48        58\n",
            "          27       0.26      0.15      0.19        66\n",
            "          28       0.54      0.55      0.55        49\n",
            "          29       0.22      0.33      0.26        64\n",
            "          30       0.51      0.35      0.42        65\n",
            "          31       0.37      0.22      0.28        68\n",
            "          32       0.26      0.29      0.28        68\n",
            "          33       0.54      0.58      0.56        64\n",
            "          34       0.14      0.10      0.12        58\n",
            "          35       0.27      0.35      0.31        51\n",
            "          36       0.22      0.28      0.25        54\n",
            "          37       0.19      0.07      0.11        68\n",
            "          38       0.19      0.14      0.16        76\n",
            "          39       0.37      0.34      0.35        65\n",
            "          40       0.33      0.28      0.30        65\n",
            "          41       0.38      0.37      0.37        71\n",
            "          42       0.11      0.06      0.08        64\n",
            "          43       0.57      0.43      0.49        65\n",
            "          44       0.64      0.86      0.74        65\n",
            "          45       0.17      0.17      0.17        58\n",
            "          46       0.42      0.36      0.38        56\n",
            "          47       0.24      0.13      0.17        70\n",
            "          48       0.26      0.22      0.24        59\n",
            "          49       0.46      0.37      0.41        57\n",
            "          50       0.34      0.24      0.28        55\n",
            "          51       0.28      0.25      0.26        65\n",
            "          52       0.39      0.26      0.31        76\n",
            "          53       0.24      0.18      0.21        55\n",
            "          54       0.34      0.27      0.30        52\n",
            "          55       0.26      0.17      0.20        59\n",
            "          56       0.69      0.72      0.70        60\n",
            "          57       0.51      0.38      0.43        61\n",
            "          58       0.46      0.48      0.47        64\n",
            "          59       0.26      0.33      0.30        54\n",
            "          60       0.25      0.32      0.28        53\n",
            "          61       0.20      0.17      0.18        53\n",
            "          62       0.37      0.37      0.37        59\n",
            "          63       0.19      0.18      0.18        62\n",
            "          64       0.36      0.40      0.38        63\n",
            "          65       0.33      0.44      0.38        55\n",
            "          66       0.29      0.56      0.38        52\n",
            "          67       0.57      0.67      0.61        69\n",
            "          68       0.28      0.20      0.24        59\n",
            "          69       0.28      0.25      0.27        59\n",
            "          70       0.78      0.82      0.80        65\n",
            "          71       0.19      0.20      0.20        44\n",
            "          72       0.37      0.23      0.29        73\n",
            "          73       0.26      0.32      0.29        68\n",
            "          74       0.26      0.26      0.26        54\n",
            "          75       0.61      0.63      0.62        62\n",
            "          76       0.71      0.81      0.76        59\n",
            "          77       0.07      0.04      0.05        56\n",
            "          78       0.10      0.08      0.09        48\n",
            "          79       0.16      0.10      0.12        72\n",
            "          80       0.49      0.51      0.50        63\n",
            "          81       0.15      0.15      0.15        60\n",
            "          82       0.57      0.46      0.51        59\n",
            "          83       0.27      0.21      0.23        62\n",
            "          84       0.24      0.41      0.31        58\n",
            "          85       0.55      0.68      0.61        65\n",
            "          86       0.73      0.86      0.79        57\n",
            "          87       0.39      0.32      0.35        65\n",
            "          88       0.25      0.25      0.25        51\n",
            "          89       0.34      0.34      0.34        58\n",
            "          90       0.34      0.21      0.26        66\n",
            "          91       0.20      0.18      0.19        55\n",
            "          92       0.15      0.11      0.13        46\n",
            "          93       0.53      0.49      0.51        75\n",
            "          94       0.27      0.19      0.22        54\n",
            "\n",
            "    accuracy                           0.33      6600\n",
            "   macro avg       0.34      0.33      0.33      6600\n",
            "weighted avg       0.33      0.33      0.32      6600\n",
            "\n",
            "Confusion Matrix:\n",
            "[[279   1   9 ...   3   3   8]\n",
            " [ 13  11   0 ...   0   1   1]\n",
            " [  7   1   8 ...   0   1   0]\n",
            " ...\n",
            " [ 15   0   0 ...   5   0   0]\n",
            " [  5   2   0 ...   0  37   0]\n",
            " [ 15   0   0 ...   1   1  10]]\n"
          ]
        }
      ]
    }
  ]
}